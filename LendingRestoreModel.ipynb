{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "269px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    },
    "colab": {
      "name": "LendingRestoreModel.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cbarlow-corelationinc/troubled_loan_detection/blob/main/LendingRestoreModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xOKgKSOlz_W"
      },
      "source": [
        "# Restoring a Saved Model#\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0KwNlIblz_Y"
      },
      "source": [
        "<a id='introduction'></a>\n",
        "## Introduction##\n",
        "This is an example of loading an already trained model from disk and using it to make predictions.\n",
        "\n",
        "<b>Table of Contents</b>\n",
        "1. [Introduction](#introduction)\n",
        "2. [Import Necessary Packages](#imports)\n",
        "3. [Load and Normalize the Data](#load_data)\n",
        "4. [Load the Model from and hdf5 Disk File](#load_model)\n",
        "5. [Summarize the Training Results](#summarize)\n",
        "6. [Conclusion](#conclusion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JInE8O4qlz_Z"
      },
      "source": [
        "<a id='imports'></a>\n",
        "## Import Necessary Packages ##\n",
        "\n",
        "First, let's run the cell below (put your cursor in the cell and press Shift-Enter) to import all the packages that you will need.\n",
        "- [numpy](https://www.numpy.org/) is the fundamental package for scientific computing with Python.\n",
        "- [keras](https://pypi.org/project/Keras/) is a high level neural network library for deep learning applications.\n",
        "- [matplotlib](http://matplotlib.org) is a famous library to plot graphs in Python. \n",
        "- [pandas](https://pandas.pydata.org/) is a data analysis library.\n",
        "- [seaborn](https://seaborn.pydata.org/) is a data visualization library based on matplotlib.\n",
        "- [scikitplot](https://anaconda.org/conda-forge/scikit-plot) is yet another plotting library.\n",
        "- [sklearn](https://scikit-learn.org/stable/) is a collection of algorithms for classification, regression, and clustering.\n",
        "- [h5py](http://www.h5py.org) is a common package to interact with a dataset that is stored on an H5 file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMaIfJymlz_Z"
      },
      "source": [
        "<b><i>Only run the following cell if you are running on Google Colab environment!<i></b> This cell is intended to install any Python libraries that are missing from the default environment in Google Colab and to pull down the necessary utility Python code and data files from Git Hub."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jg5jImUblz_a"
      },
      "source": [
        "import sys\n",
        "\n",
        "if sys.platform.startswith('linux'):\n",
        "    # Extra imports for pulling zip file from Carl GitHub repository.\n",
        "    import requests, zipfile, io\n",
        "\n",
        "    # Install additional Python libraries that are missing from the default environment in Google Colab\n",
        "    %pip install scikit-plot\n",
        "\n",
        "    # Remove folders that may be left over from previous runs\n",
        "    !rm -f datasets/*\n",
        "    !rmdir datasets\n",
        "    !rm -f helper_code/*\n",
        "    !rmdir helper_code\n",
        "    !rm -f models/*\n",
        "    !rmdir models\n",
        "\n",
        "    # Get necessary Python code and data files from Git Hub repository\n",
        "    r = requests.get( 'https://raw.github.com/cbarlow-corelationinc/troubled_loan_detection/main/TroubledLoanDetectionFiles.zip')\n",
        "    r.status_code\n",
        "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "    z.extractall()\n",
        "    !ls -Abl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fh5QFhs_lz_c"
      },
      "source": [
        "# Import packages\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "import pandas as pd\n",
        "from pandas.api.types import CategoricalDtype\n",
        "from sklearn.model_selection import train_test_split\n",
        "from cb3_ann_utils import describe_datasets, build_compile_fit_model, \\\n",
        "    my_f1_score, cb3_plot_confusion_matrix, cb3_calculateResults1, cb3_calculateResults2, cb3_displayResults\n",
        "\n",
        "print(\"Done importing required modules.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5We6dx-Clz_d"
      },
      "source": [
        "<a id='load_data'></a>\n",
        "## Load and Normalize the Data ##\n",
        "\n",
        "Use pandas.read_csv() function to read the loan data from a CSV disk files into a Pandas DataFrame, and then shuffle and split up the data into a training set, a dev set (also known as a \"cross validation\" or a \"holdout\" set), and a test set.  Training is only done on the training set.  The dev set it used to evaluate the ability of the trained network to generalize well and can be used to help \"tune\" hyperparameter settings.  Finally, the test set is used to see if the performance is relatively consistent between the dev set and the test set, so that we can have confidence that the test set performance is an accurate representation of how well the network will perform on data that was not seen during training (assuming the training data was a good representative sample).  NOTE: THE FIELDS IN THE DATA FILE MUST MATCH 100% THE FIELDS THAT WERE USED AS FEATURES WHILE TRAINING THE MODEL THAT WE WILL USE!  Also, when running future predictions on unlabeled data, you will need to know the original mean and standard deviation that were used to normalize the training data so they can also be applied to new data.  So in a production product, save the mean and standard deviation of the training data along with the model itself."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qyywbPXlz_d"
      },
      "source": [
        "np.random.seed(31)    # in case you need to be able to duplicate exact shuffling of data\n",
        "\n",
        "# Read CSV data\n",
        "loan_data = pd.read_csv('datasets/AnonymousLoans20190930OregonStateFull.csv', index_col = 'LOAN_SERIAL')\n",
        "\n",
        "# Extract and encode categorical features\n",
        "ln_type_cat = loan_data[['LN_TYPE_CATEGORY']].astype(CategoricalDtype([\"CC\", \"CE\", \"LC\", \"OE\"]))\n",
        "ln_type_cat = pd.get_dummies(ln_type_cat, prefix='LN_TYPE_CAT', drop_first=True)\n",
        "ln_type_cat.head()\n",
        "\n",
        "categorical_features = ln_type_cat\n",
        "\n",
        "# Extract numerical features\n",
        "#numerical_features = loan_data[['CREDIT_LIMIT', 'COBORROWER_COUNT', 'ORIGINAL_TERM_IN_MONTHS', 'BALANCE', 'MONTHLY_PMT', 'INTEREST_RATE', \\\n",
        "#                               'DQ_DAYS', 'STILL_MISSED_FIRST_PMT', 'HAS_COLLATERAL', 'AGE_IN_DAYS', 'MEMBERSHIP_DAYS', 'PMT_COUNT_MADE', \\\n",
        "#                               'PMT_COUNT_DQ_30_TO_59', 'PMT_COUNT_DQ_60_TO_89', 'PMT_COUNT_DQ_90_TO_119', \\\n",
        "#                               'PMT_COUNT_DQ_120_AND_UP', 'MONTHLY_INCOME', 'MONTHLY_EXPENSE', 'LR_AMOUNT', 'SCORE', 'AGE_OF_SCORE', \\\n",
        "#                               'AGGR_SHARE_COUNT', 'AGGR_SHARE_BAL', 'DD_CHANGE', 'CREDIT_LIMIT_INCREASE_COUNT', 'OFF_AUTO_PAY_COUNT']]\n",
        "numerical_features = loan_data[['DQ_DAYS', 'AGGR_SHARE_BAL', 'PMT_COUNT_MADE', 'MONTHLY_PMT', 'PMT_COUNT_DQ_30_TO_59', \\\n",
        "                                'CREDIT_LIMIT', 'INTEREST_RATE', 'PMT_COUNT_DQ_90_TO_119', 'SCORE', \\\n",
        "                                'COBORROWER_COUNT', 'MONTHLY_EXPENSE']]\n",
        "#loan_features = pd.concat([categorical_features, numerical_features], axis=1)\n",
        "loan_features = numerical_features\n",
        "\n",
        "\n",
        "loan_labels = loan_data['BINARY_STATUS']\n",
        "frame = { 'BINARY_STATUS': loan_labels } \n",
        "loan_labels_df = pd.DataFrame(frame)\n",
        "\n",
        "# Split the data up in train and dev sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(loan_features, loan_labels_df, train_size=0.8)\n",
        "# Further split dev data into dev and test sets\n",
        "X_dev, X_test, y_dev, y_test = train_test_split(X_test, y_test, test_size=0.50)\n",
        "\n",
        "# Normalize the data\n",
        "train_mean = X_train.mean(axis=0)\n",
        "train_std = X_train.std(axis=0)\n",
        "train_std = np.where(train_std==0.0, 1.e-8, train_std) # avoid divide by zero error\n",
        "X_train -= train_mean\n",
        "X_train /= train_std\n",
        "# scale dev data and test data based on train data mean and standard deviation!\n",
        "X_dev -= train_mean\n",
        "X_dev /= train_std\n",
        "X_test -= train_mean\n",
        "X_test /= train_std\n",
        "    \n",
        "n = X_train.shape[1]  # feature count\n",
        "#describe_datasets(X_train, y_train, X_dev, y_dev, X_test, y_test, n)\n",
        "describe_datasets(X_train, y_train, X_dev, y_dev, X_test, y_test, n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYmZ8jpxlz_e"
      },
      "source": [
        "<a id='load_model'></a>\n",
        "## Load the Model from an hdf5 Disk File ##\n",
        "The model has already been trained previously, so now you can just load the model from an hdf5 disk file, and then run the summary() function to display the neural network architecture used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHcUNRnFlz_h"
      },
      "source": [
        "# load model\n",
        "model = load_model('./models/model_20200831_s31a.h5')\n",
        "# summarize model.\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gR_mPe7Xlz_i"
      },
      "source": [
        "<a id='summarize'></a>\n",
        "## Summarize the Training Results ##\n",
        "Display summary of various statistics about the performance of the model against the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaWqu-_Plz_i"
      },
      "source": [
        "# Now make predictions on test data\n",
        "y_pred_test = model.predict(X_test)\n",
        "y_pred_test[y_pred_test>=0.5] = 1\n",
        "y_pred_test[y_pred_test<0.5] = 0\n",
        "y_test_numpy = y_test.to_numpy()\n",
        "\n",
        "(tp, tn, fp, fn) = cb3_calculateResults1(y_pred_test, y_test_numpy)\n",
        "(accuracy, precision, recall, f1, cm) = cb3_calculateResults2(tp, tn, fp, fn)\n",
        "cb3_displayResults(accuracy, precision, recall, f1, cm, \"Test Set\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zD-k5VpBlz_j"
      },
      "source": [
        "Here are what the results would look like if we just set p(badLoan)=0:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "h4FDSjj6lz_j"
      },
      "source": [
        "bad = tp + fn\n",
        "good = tn + fp\n",
        "totLoans = bad + good\n",
        "(accuracy, precision, recall, f1, cm) = cb3_calculateResults2(0, good, 0, bad)\n",
        "cb3_displayResults(accuracy, precision, recall, f1, cm, \"p(BadLoans)=0\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuZr13TNlz_j"
      },
      "source": [
        "And here are what the results would look like if we just randomly labeled a small number as bad (p(badLoan) = actual percentage bad) in hopes of improving accuracy from the p(badLoan) = 0 model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "HrTADc7slz_k"
      },
      "source": [
        "p = bad\n",
        "n = good\n",
        "tp = int(round(p * bad / totLoans))\n",
        "fp = p - tp\n",
        "tn = int(round(n * good / totLoans))\n",
        "fn = n - tn\n",
        "(accuracy, precision, recall, f1, cm) = cb3_calculateResults2(tp, tn, fp, fn)\n",
        "cb3_displayResults(accuracy, precision, recall, f1, cm, \"p(BadLoans)=observedProbability\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwDvVO_slz_k"
      },
      "source": [
        "You can also look at roughly which input features had the most-to-least influence on predictions (take this simplistic interpretation of the weights in the first hidden layer with a grain of salt):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xjqShyulz_l"
      },
      "source": [
        "weights = np.sum(np.absolute(model.layers[0].get_weights()[0]), axis=1)\n",
        "print(\"Weights in original feature order:\")\n",
        "print(weights)\n",
        "wi = np.argsort(weights)\n",
        "ws = sorted(weights)\n",
        "print(\"\\nWeights sorted most to least influential:\")\n",
        "for i in range(len(wi)-1, -1, -1):\n",
        "    print(ws[i],X_train.columns.values[wi[i]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_WACz3Dlz_m"
      },
      "source": [
        "We can also go back and look at a few examples of false negatives (where we predicted a good loan in the dev data, but it was actually bad), getting the full, non-normalized data from original_data. Then we can go back to the credit union and find these loans and examine them for further \"inspiration\" on what additional features might have made this more predictable. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2QqJnnJlz_n"
      },
      "source": [
        "# Now look at up to three examples of false negatives for inspiration\n",
        "false_negative_tuple = np.where(((y_pred_test==0) & (y_test_numpy==1)))\n",
        "false_negative_array = false_negative_tuple[0]\n",
        "l = len(false_negative_array)\n",
        "if l>0:\n",
        "    print(\"\\nExamples of false negatives in test data:\")\n",
        "for i in range(min(l, 3)):\n",
        "    loan_serial = X_test.index.values[false_negative_array[i]]\n",
        "    print(\"\\nLoan Serial \",loan_serial)\n",
        "    print(loan_data.loc[loan_serial])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9S-GGwNlz_n"
      },
      "source": [
        "Or we can list the serial numbers of <i>all</i> false negatives:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JCksmu7lz_o"
      },
      "source": [
        "if l>0:\n",
        "    print(\"\\nFalse negatives in test data:\")\n",
        "for i in range(l):\n",
        "    loan_serial = X_test.index.values[false_negative_array[i]]\n",
        "    print(\"Loan Serial \",loan_serial)\n",
        "    #print(loan_data.loc[loan_serial])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tc0FigFFlz_o"
      },
      "source": [
        "<a id='conclusion'></a>\n",
        "## Conclusion  ##\n",
        "It takes much less time to make predictions using an existing, trained model, than it does to train a model in the first place.\n",
        "\n",
        "\n",
        "[Return to top of Notebook](#introduction)"
      ]
    }
  ]
}