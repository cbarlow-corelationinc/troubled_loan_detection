{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "175px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    },
    "colab": {
      "name": "TroubledLoanDetection_ANN.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cbarlow-corelationinc/troubled_loan_detection/blob/main/TroubledLoanDetection_ANN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPDeQ74xWOx_"
      },
      "source": [
        "# Troubled Loan Detection with [Deep Learning](#https://en.wikipedia.org/wiki/Deep_learning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvKFgEp_WOyH"
      },
      "source": [
        "<a id='introduction'></a>\n",
        "## Introduction##\n",
        "This [Jupyter notebook](https://jupyter.org/) is an example of how \"Deep Learning\" (a branch of machine learning that uses Artificial Neural Networks) can analyze features and labels of a training set to try to \"learn\" a set of weights for being able to \"predict\" the labels in additional data that was not used during training. For machine learning to work, we are making some important assumptions that may not be true for every use case.  In particular, we are hypothesizing (a) that the outputs <i>can</i> be predicted from the inputs, and (b) that the available data is sufficiently informative to learn the relationship between the inputs and outputs.  These two assumptions largely depend on the input features we select and the amount of representational data we can collect.\n",
        "\n",
        "For this particular example we will examine over 30 loan features on roughly 75,000 loans that have all been labeled as 1 (\"bad\") or 0 (\"good\"), based on whether or not they were charged off within six months of the date the data was examined.  The methods used employ the [keras](https://keras.io/) library for [Python](https://en.wikipedia.org/wiki/Python_(programming_language)).  This is an example of [supervised learning](https://en.wikipedia.org/wiki/Supervised_learning) (we provide the \"ground truth\" labels for the training data), using various models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJz9T62-WOyI"
      },
      "source": [
        "<b><i>Only run the following cell if you are running on Google Colab environment!<i></b> This cell is intended to install any Python libraries that are missing from the default environment in Google Colab and to pull down the necessary utility Python code and data files from Git Hub."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpK9Sy1mWOyK"
      },
      "source": [
        "import sys\n",
        "\n",
        "if sys.platform.startswith('linux'):\n",
        "    # Extra imports for pulling zip file from Carl GitHub repository.\n",
        "    import requests, zipfile, io\n",
        "\n",
        "    # Install additional Python libraries that are missing from the default environment in Google Colab\n",
        "    %pip install scikit-plot\n",
        "\n",
        "    # Remove folders that may be left over from previous runs\n",
        "    !rm -f datasets/*\n",
        "    !rmdir datasets\n",
        "    !rm -f helper_code/*\n",
        "    !rmdir helper_code\n",
        "    !rm -f models/*\n",
        "    !rmdir models\n",
        "\n",
        "    # Get necessary Python code and data files from Git Hub repository\n",
        "    r = requests.get( 'https://raw.github.com/cbarlow-corelationinc/troubled_loan_detection/main/TroubledLoanDetectionFiles.zip')\n",
        "    r.status_code\n",
        "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "    z.extractall()\n",
        "    !ls -Abl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APrd_56AWOyL"
      },
      "source": [
        "<a id='imports'></a>\n",
        "## Import Necessary Packages ##\n",
        "\n",
        "First, let's run the cell below (put your cursor in the cell and press Shift-Enter) to import all the packages that you will need.\n",
        "- [numpy](https://www.numpy.org/) is the fundamental package for scientific computing with Python. \n",
        "- [pandas](https://pandas.pydata.org/) is a data analysis library.\n",
        "- [matplotlib](http://matplotlib.org) is a famous library to plot graphs in Python.\n",
        "- [seaborn](https://seaborn.pydata.org/) is a data visualization library based on matplotlib.\n",
        "- [scikitplot](https://anaconda.org/conda-forge/scikit-plot) is yet another plotting library.\n",
        "- [sklearn](https://scikit-learn.org/stable/) is a collection of algorithms for classification, regression, and clustering.\n",
        "- [keras](https://pypi.org/project/Keras/) is a high level neural network library for deep learning applications.\n",
        "- [h5py](http://www.h5py.org) is a common package to interact with a dataset that is stored on an H5 file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eu3fipFcWOyM"
      },
      "source": [
        "# Standard imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# We do this to ignore several specific Pandas warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Set default seaborn plotting style\n",
        "sns.set(style='white', context='notebook', palette='deep')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5hwY-ixWOyN"
      },
      "source": [
        "And some additional imports we will need for these models and metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ov3OfQ_6WOyN"
      },
      "source": [
        "#import tensorflow.python.util.deprecation as deprecation\n",
        "\n",
        "from datetime import datetime\n",
        "from math import log10\n",
        "from pandas.api.types import CategoricalDtype\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import resample\n",
        "\n",
        "from cb3_ann_utils import describe_datasets, build_compile_fit_model, \\\n",
        "    my_f1_score, cb3_calculateResults1, cb3_calculateResults2, cb3_displayResults\n",
        "from cb3_email_util import email_result, email_completion_notice\n",
        "\n",
        "# Outdated imports from earlier version of notebook\n",
        "# Helper code to plot confusion matrix\n",
        "#from helper_code import mlplots as ml\n",
        "# Set global figure properties\n",
        "#import matplotlib as mpl\n",
        "#mpl.rcParams.update({'axes.titlesize' : 20,\n",
        "#                     'axes.labelsize' : 18,\n",
        "#                     'legend.fontsize': 16})\n",
        "%matplotlib inline\n",
        "\n",
        "print(\"Done importing required modules.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdgfgWL9WOyN"
      },
      "source": [
        "Return to [top](#introduction)\n",
        "<a id='load_data'></a>\n",
        "## Load the Data\n",
        "The Pandas library has a read_csv() function that can read a CSV file directly into a Pandas DataFrame. This is particularly easy if the first row of the CSV file has labels and if those labels do not contain spaces. We can even specify a column that will act as an index and not as a normal column, as seen here with the LOAN_SERIAL column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3aF8rXxWOyO"
      },
      "source": [
        "# Read CSV data\n",
        "loan_data = pd.read_csv('datasets/AnonymousLoans20190930OregonStateFull.csv', index_col = 'LOAN_SERIAL')\n",
        "loan_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHJQK1_CWOyO"
      },
      "source": [
        "Return to [top](#introduction)\n",
        "<a id='explore_data'></a>\n",
        "## Exploratory Data Analysis (EDA) (Optional) ##\n",
        "Now you can optionally display additional information about the data, such as the keys/feature column names, how many non-null values there are for each, and the data type (float64):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7ll4POXWOyQ"
      },
      "source": [
        "loan_data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaVdHdqnWOyQ"
      },
      "source": [
        "Return to [top](#introduction)\n",
        "<a id='encode'></a>\n",
        "## Encode Categorical Features ##\n",
        "Now do a one-hot label encoding on the categorical feature LN_TYPE_CATEGORY and split the dataframe into loan_features and loan_labels, where the label comes from the BINARY_STATUS column, where 1 indicates a loan that gets charged off in next six months and 0 inidicates the majority case where the loan is <i>not</i> charged off in the next six months."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5tlzgyyWOyQ"
      },
      "source": [
        "ln_type_cat = loan_data[['LN_TYPE_CATEGORY']].astype(CategoricalDtype([\"CC\", \"CE\", \"LC\", \"OE\"]))\n",
        "ln_type_cat = pd.get_dummies(ln_type_cat, prefix='LN_TYPE_CAT', drop_first=True)\n",
        "ln_type_cat.head()\n",
        "\n",
        "categorical_features = ln_type_cat\n",
        "\n",
        "# Extract numerical features\n",
        "#numerical_features = loan_data[['CREDIT_LIMIT', 'COBORROWER_COUNT', 'ORIGINAL_TERM_IN_MONTHS', 'BALANCE', 'MONTHLY_PMT', 'INTEREST_RATE', \\\n",
        "#                               'DQ_DAYS', 'STILL_MISSED_FIRST_PMT', 'HAS_COLLATERAL', 'AGE_IN_DAYS', 'MEMBERSHIP_DAYS', 'PMT_COUNT_MADE', \\\n",
        "#                               'PMT_COUNT_DQ_30_TO_59', 'PMT_COUNT_DQ_60_TO_89', 'PMT_COUNT_DQ_90_TO_119', \\\n",
        "#                               'PMT_COUNT_DQ_120_AND_UP', 'MONTHLY_INCOME', 'MONTHLY_EXPENSE', 'LR_AMOUNT', 'SCORE', 'AGE_OF_SCORE', \\\n",
        "#                               'AGGR_SHARE_COUNT', 'AGGR_SHARE_BAL', 'DD_CHANGE', 'CREDIT_LIMIT_INCREASE_COUNT', 'OFF_AUTO_PAY_COUNT']]\n",
        "# using only 11 of original 30 features\n",
        "numerical_features = loan_data[['DQ_DAYS', 'AGGR_SHARE_BAL', 'PMT_COUNT_MADE', 'MONTHLY_PMT', 'PMT_COUNT_DQ_30_TO_59', \\\n",
        "                                'CREDIT_LIMIT', 'INTEREST_RATE', 'PMT_COUNT_DQ_90_TO_119', 'SCORE', \\\n",
        "                                'COBORROWER_COUNT', 'MONTHLY_EXPENSE']]\n",
        "#loan_features = pd.concat([categorical_features, numerical_features], axis=1)\n",
        "loan_features = numerical_features\n",
        "\n",
        "loan_labels = loan_data['BINARY_STATUS']\n",
        "frame = { 'BINARY_STATUS': loan_labels } \n",
        "loan_labels_df = pd.DataFrame(frame)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiC0c9yfWOyQ"
      },
      "source": [
        "cor = loan_features.corr()\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.heatmap(cor,cmap='Set1',annot=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BHMSsFbWOyR"
      },
      "source": [
        "Return to [top](#introduction)\n",
        "<a id='split_and_normalize'></a>\n",
        "## Split and Normalize the Data Set\n",
        "Now let's try a Support Vector Machine (SVM). We will start over from the original loan_features (this time we will use a one-hot encoding of LN_TYPE_CATEGORY) and loan_labels, and split them into train, dev, and test sets. We will make the training set be 80% of the total loans, and we will divide the remaining loans equally between the dev and test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9Se2jF5WOyR"
      },
      "source": [
        "# Extract and encode categorical features\n",
        "rs = 31\n",
        "np.random.seed(rs)\n",
        "\n",
        "# Split the data up in train and dev sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(loan_features, loan_labels_df, train_size=0.8)\n",
        "# Further split dev data into dev and test sets\n",
        "X_dev, X_test, y_dev, y_test = train_test_split(X_test, y_test, test_size=0.50)\n",
        "\n",
        "# Normalize the data\n",
        "train_mean = X_train.mean(axis=0)\n",
        "train_std = X_train.std(axis=0)\n",
        "train_std = np.where(train_std==0.0, 1.e-8, train_std) # avoid divide by zero error\n",
        "X_train -= train_mean\n",
        "X_train /= train_std\n",
        "# scale dev data and test data based on train data mean and standard deviation!\n",
        "X_dev -= train_mean\n",
        "X_dev /= train_std\n",
        "X_test -= train_mean\n",
        "X_test /= train_std\n",
        "    \n",
        "n = X_train.shape[1]  # feature count\n",
        "describe_datasets(X_train, y_train, X_dev, y_dev, X_test, y_test, n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnKmUdDEWOyR"
      },
      "source": [
        "Return to [top](#introduction)\n",
        "<a id='oversample_minority'></a>\n",
        "## Oversample the Minority Case (Optional) ##\n",
        "This section is optional.  If you choose, you can skip to the section where you [prepare the model](#prepare_model).\n",
        "\n",
        "The dataset is very skewed towards \"good\" loans (over 98%!).  One technique that is prevalent for improving the training on the dataset is to \"oversample\" the minority set (\"bad\" loans) so that they are better represented, although this can make overfitting more likely, so guard against that with regularization, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "OI2wCdu9WOyS"
      },
      "source": [
        "X=pd.concat([X_train,y_train],axis=1)\n",
        "\n",
        "good_loans=X[X.BINARY_STATUS==0]\n",
        "bad_loans=X[X.BINARY_STATUS==1]\n",
        "\n",
        "# upsample minority\n",
        "bad_loans_oversampled = resample(bad_loans,\n",
        "                          replace=True, # sample with replacement\n",
        "                          n_samples=len(good_loans)//4, # match one fourth of number in majority class\n",
        "                          random_state=rs) # reproducible results\n",
        "\n",
        "# combine majority and upsampled minority\n",
        "oversampled = pd.concat([good_loans, bad_loans_oversampled])\n",
        "# reshuffle the dataset\n",
        "oversampled = oversampled.sample(frac=1).reset_index(drop=True)\n",
        "X_train = oversampled.iloc[:,:-1]\n",
        "#y_train = oversampled.iloc[:,-1]\n",
        "y_train = oversampled[['BINARY_STATUS']].copy()\n",
        "del oversampled  # delete combined, oversampled training data now that it is divided again into X_train and y_train\n",
        "\n",
        "# check new class counts\n",
        "print(\"\\nOversampling minority case results in the following new distribution of labels in test data:\")\n",
        "print(y_train.BINARY_STATUS.value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUlEiE8ZWOyS"
      },
      "source": [
        "Return to [top](#introduction)\n",
        "<a id='prepare_model'></a>\n",
        "## Prepare the Model by Setting the Hyperparameters ##\n",
        "Now set the hyperparamters, including how many times to iterate through the learning process, setting some of the hyperparameters randomly.  Set iterateCount = 1 if you just want to train on the data one time with the assigned hyperparameter values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnmkxrXHWOyS"
      },
      "source": [
        "# Now set hyperparameters before you build, compile, and fit model\n",
        "learning_rate = 0.0777\n",
        "regularization_factor = 0.0000018\n",
        "\n",
        "hidden_layer_count = 3\n",
        "units_per_hidden_layer = 5\n",
        "send_progress_emails = False  # Emails sent to carl.barlow.iii@gmail.com\n",
        "print_summary = True   # display network architecture\n",
        "\n",
        "epochs = 5\n",
        "batch_size=256\n",
        "batch_normalization=True\n",
        "verbose = 0     # 0 = silent; 1 = progress bar; 2 = one line per epoch\n",
        "\n",
        "beta_1 = 0.9    # hyperparameter for Adam optimization\n",
        "beta_2 = 0.999  # hyperparameter for Adam optimization\n",
        "epsilon = 1.e-8 # hyperparameter for Adam optimization to avoid divide by zero\n",
        "decay = 0.0     # learning rate decay\n",
        "amsgrad = False\n",
        "loss_function='binary_crossentropy'  # appropriate for binary classification problems\n",
        "\n",
        "iterateCount = 50  # set to number greater than 1 if wanting to tune learning_rate, regularization_factor, and units_per_hidden_layer\n",
        "lr_ten_min_exp = log10(0.030)    # choose learning rates from 0.030 to...\n",
        "lr_ten_max_exp = log10(0.120)    # ...0.120, chosen uniformly along log scale\n",
        "rf_ten_min_exp = log10(0.0000001)  # choose regularization_factor from 0.0000001 to...\n",
        "rf_ten_max_exp = log10(0.0000300)  # ...0.0000300, chosen uniformly along log scale\n",
        "hu_min = 3      # choose units_per_hidden_layer integer from 8 to...\n",
        "hu_max = 12     # ...14 (inclusive) chosen uniformly in range\n",
        "\n",
        "print(\"\\nHyperparameters have been set.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sCG-sI3WOyS"
      },
      "source": [
        "Return to [top](#introduction)\n",
        "<a id='iterate'></a>\n",
        "## Iteratively Build, Compile, and Fit (train) the Model ##\n",
        "Now iterate through various random values for learning rate, regularization factor, and hidden units per layer to find best combination of these. In this case, we will use the F1 score of the dev set as the metric.  Careful: if you use the dev set too much to tune your hyperparameters, you can overfit your dev set, as will be evidenced by worse performance against the test set.  Can be useful to use a course-to-fine approach to narrow in on best values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d44XOJdKWOyS"
      },
      "source": [
        "best_lr = None\n",
        "best_units_per_hidden_layer = None\n",
        "best_rf = None\n",
        "\n",
        "best_f1 = 0.0\n",
        "best_model = None\n",
        "best_history = None\n",
        "\n",
        "startTime = datetime.now()\n",
        "print(\"Start time: \", startTime)\n",
        "for i in range(iterateCount):\n",
        "    if (i % 10 == 0):     # periodically show progress as models are trained\n",
        "        print(\"\\nStarting iteration \", i)\n",
        "        timeElapsed = datetime.now() - startTime\n",
        "        print('Time elapsed (hh:mm:ss.ms) {}'.format(timeElapsed))\n",
        "        \n",
        "    if iterateCount>1:\n",
        "        learning_rate = 10**np.random.uniform(lr_ten_min_exp,lr_ten_max_exp)          # random learning_rate distributed uniformly on a logarithmic scale\n",
        "        regularization_factor = 10**np.random.uniform(rf_ten_min_exp,rf_ten_max_exp)  # random regularization_factor distributed uniformly on a logarithmic scale\n",
        "        units_per_hidden_layer = np.random.randint(hu_min,hu_max+1)                   # units uniformly set to integer from 18 to 22 inclusive\n",
        "    \n",
        "    # This is the line that really does the learning.  It is only inside a loop for tuning hyperparameters\n",
        "    model, history = build_compile_fit_model(X_train, y_train, validation_data=(X_dev, y_dev), feature_count=n, regularization_factor=regularization_factor, \n",
        "                                             hidden_layer_count=hidden_layer_count, units_per_hidden_layer=units_per_hidden_layer,  \n",
        "                                             lr=learning_rate, beta_1=beta_1, beta_2=beta_2, epsilon=epsilon, decay=decay,  amsgrad=amsgrad, \n",
        "                                             loss_function=loss_function, epochs=epochs, batch_size=batch_size, \n",
        "                                             batch_normalization=batch_normalization, verbose=verbose) \n",
        "\n",
        "    f1 = my_f1_score(X_dev, y_dev, model)  # use instead of sklearn.metrics.f1_score(y_dev, y_pred) \n",
        "    if (i==0 or f1>best_f1):  # save off and display details if this is best f1 score so far\n",
        "        best_f1 = f1\n",
        "        best_lr = learning_rate\n",
        "        best_rf = regularization_factor\n",
        "        best_units_per_hidden_layer = units_per_hidden_layer\n",
        "        best_model = model\n",
        "        best_history = history\n",
        "        \n",
        "        print(\"\\nOn iteration \", i)\n",
        "        print(\"Best learning_rate: \", best_lr)\n",
        "        print(\"Best regularization_factor: \", best_rf)\n",
        "        print(\"Best units_per_hidden_layer: \", best_units_per_hidden_layer)\n",
        "        print(\"Best dev set f1 score: \", best_f1)\n",
        "        if send_progress_emails:\n",
        "            email_result(i, best_lr, best_rf, best_units_per_hidden_layer, best_f1)\n",
        "        model.save(\"./models/model.h5\")\n",
        "        print(\"Saved model to disk\")\n",
        "        \n",
        "timeElapsed = datetime.now() - startTime\n",
        "print('\\nFinished!  Total time elapsed (hh:mm:ss.ms) {}'.format(timeElapsed))\n",
        "if send_progress_emails:\n",
        "    email_completion_notice()\n",
        "\n",
        "learning_rate = best_lr\n",
        "regularization_factor = best_rf\n",
        "units_per_hidden_layer = best_units_per_hidden_layer\n",
        "model = best_model\n",
        "history = best_history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ionCeBNvWOyT"
      },
      "source": [
        "Return to [top](#introduction)\n",
        "<a id='summarize'></a>\n",
        "## Summarize the Results ##\n",
        "Display summary of the selected network architecture (optional), and display various statistics about the performance of the best model found during the iterations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "35ajkq1LWOyT"
      },
      "source": [
        "# optionally summarize the network architecture\n",
        "if (print_summary):\n",
        "    model.summary()\n",
        "       \n",
        "print(\"Best learning rate: \", learning_rate)\n",
        "print(\"Best regularization_factor: \", regularization_factor)\n",
        "print(\"Best units_per_hidden_layer: \", units_per_hidden_layer)\n",
        "print(\"Best dev set f1 score: \", best_f1)\n",
        "\n",
        "history_dict = history.history\n",
        "train_loss = history_dict['loss']\n",
        "dev_loss = history_dict['val_loss']\n",
        "# Breaking change in Keras 2.3.0 changed key in history from 'acc' to 'accuracy' and from 'val_acc' to 'val_accuracy'.\n",
        "train_acc = history_dict['accuracy']\n",
        "dev_acc = history_dict['val_accuracy']\n",
        "epochs = range(1, len(dev_loss) + 1)\n",
        "\n",
        "# Now display two graphs side by side to compare loss and accuracy for training vs. validation set\n",
        "fig, (loss_axis, acc_axis) = plt.subplots(nrows=1, ncols=2, figsize=(12,4))\n",
        "loss_axis.plot(epochs, train_loss, 'b', label='Training loss')\n",
        "loss_axis.plot(epochs, dev_loss, 'r', label='Validation loss')\n",
        "loss_axis.set_title('Training and Validation Loss')\n",
        "loss_axis.set_xlabel('Epochs')\n",
        "loss_axis.set_ylabel('Loss')\n",
        "\n",
        "acc_axis.plot(epochs, train_acc, 'b', label='Training accuracy')\n",
        "acc_axis.plot(epochs, dev_acc, 'r', label='Validation accuracy')\n",
        "acc_axis.set_title('Training and Validation Accuracy')\n",
        "acc_axis.set_xlabel('Epochs')\n",
        "acc_axis.set_ylabel('Accuracy')\n",
        "plt.show()\n",
        "\n",
        "final_train_acc = model.evaluate(X_train, y_train, verbose=0)[1]\n",
        "print(\"Final Training Set accuracy: \", final_train_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcwBLVthWOyU"
      },
      "source": [
        "Using the best model found, make predictions on the dev and test set, and output different measurements, including a confusion matrix plot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "KxQjc78QWOyU"
      },
      "source": [
        "# Now make predictions on dev data\n",
        "y_pred_dev = model.predict(X_dev)\n",
        "y_pred_dev[y_pred_dev>=0.5] = 1\n",
        "y_pred_dev[y_pred_dev<0.5] = 0\n",
        "y_dev_numpy = y_dev.to_numpy()\n",
        "\n",
        "(tp, tn, fp, fn) = cb3_calculateResults1(y_pred_dev, y_dev_numpy)\n",
        "(accuracy, precision, recall, f1, cm) = cb3_calculateResults2(tp, tn, fp, fn)\n",
        "cb3_displayResults(accuracy, precision, recall, f1, cm, \"Dev Set\")\n",
        "\n",
        "# Now make predictions on test data\n",
        "y_pred_test = model.predict(X_test)\n",
        "y_pred_test[y_pred_test>=0.5] = 1\n",
        "y_pred_test[y_pred_test<0.5] = 0\n",
        "y_test_numpy = y_test.to_numpy()\n",
        "\n",
        "# calculate count of true positives, true negatives, false positives, and false negatives\n",
        "(tp, tn, fp, fn) = cb3_calculateResults1(y_pred_test, y_test_numpy)\n",
        "# calculate accuracy, precision, recall, f1 score, and a confusion matrix\n",
        "(accuracy, precision, recall, f1, cm) = cb3_calculateResults2(tp, tn, fp, fn)\n",
        "cb3_displayResults(accuracy, precision, recall, f1, cm, \"Test Set\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_ct6gIyWOyU"
      },
      "source": [
        "You can also look at roughly which input features had the most-to-least influence on predictions (take this simplistic interpretation of the weights in the first hidden layer with a grain of salt):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofRoi8MfWOyU"
      },
      "source": [
        "weights = np.sum(np.absolute(model.layers[0].get_weights()[0]), axis=1)\n",
        "print(\"Weights in original feature order:\")\n",
        "print(weights)\n",
        "wi = np.argsort(weights)\n",
        "ws = sorted(weights)\n",
        "print(\"\\nWeights sorted most to least influential:\")\n",
        "for i in range(len(wi)-1, -1, -1):\n",
        "    print(ws[i],X_train.columns.values[wi[i]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4pYBBFwWOyU"
      },
      "source": [
        "We can also go back and look at a few examples of false negatives (where we predicted a good loan in the dev data, but it was actually bad), getting the full, non-normalized data from original_data. Then we can go back to the credit union and find this loan and examine it for further \"inspiration\" on what additional features might have made this more predictable. Remember that the features have been normalized, so you're really better off just using the loan serials to do additional research on the credit union database."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "hudhtWMfWOyU"
      },
      "source": [
        "# Now look at up to three examples of false negatives for inspiration\n",
        "false_negative_tuple = np.where(((y_pred_dev==0) & (y_dev_numpy==1)))\n",
        "false_negative_array = false_negative_tuple[0]\n",
        "l = len(false_negative_array)\n",
        "if l>0:\n",
        "    print(\"\\nExamples of false negatives:\")\n",
        "for i in range(min(l, 3)):\n",
        "    loan_serial = X_dev.index.values[false_negative_array[i]]\n",
        "    print(\"\\nLoan Serial \",loan_serial)\n",
        "#    print(X_dev.loc[loan_serial])\n",
        "    print(loan_data.loc[loan_serial])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfJk2zk8WOyV"
      },
      "source": [
        "If you want, you can return to the section how [preparing the model](#prepare_model) to adjust your hyperparameters and then iterate again."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNtrBt-zWOyV"
      },
      "source": [
        "<a id='conclusion'></a>\n",
        "## Conclusion  ##\n",
        "There are several different areas of Artificial Intelligence.  The area of Machine Learning--and particularly Supervised Machine Learning using Artificial Neural Networks with multiple hidden layers (so-called \"Deep Learning\")--has gained a lot of traction with recent developments in computing power, algorithms, and available data.  While we can not expect to predict with 100% accuracy any given outcome based on some set of input features, it does appear based on experimentation that we can train a neural network on the loan features demonstrated above and do a good job of screening loans that should be reviewed for possible problems.  This is only <i>one</i> possible use case for machine learning, but if it can help a credit union cut their chargeoffs around 40% by identifying early loans that might benefit from some sort of proactive \"wellness\" program, that should be of benefit to our clients.  Ideally, the training should be done separately for each client (and even be retrained periodically per client), although for converting clients who have not been on our system long enough to acquire the required history or that are too small to build up a large enough training set, it is feasible to start with an already trained network to do the initial predictions.  However, some of the input features are based on some recent historical trends, such as changes in direct deposit amount, going from automatic payments to cash, etc., so some allowances will need to be made for any clients who have not been on the system at least six months.\n",
        "\n",
        "<b>Recommendations for next steps (some of which can be done in parallel):</b>\n",
        "- Bring John and Jeff into the loop.  Work with John to load Python 3 on our development AIX server, along with the modules we already know we will need.  Might also look into how to run a Jupyter notebook server on AIX for training and collaborating.\n",
        "- Carl try to duplicate machine learning results from laptop on the AIX server.\n",
        "- Find a way to survey developers on their familiarity/experience with AI/Machine Learning/Neural Networks and with Python, while making it clear the need for discretion.\n",
        "- Identify one or two developers who would be good candidates for additional training in Machine Learning.  Plan and implement a curriculum with specific goals and deadlines.\n",
        "- <del>Also, complete last two courses in Deep Learning specialization.  It seems unlikely to me that Convolutional Networks have any applicability, but the course on “Sequence Models” covers Recurrent Networks, and I think those might be applicable to some use cases for us, so I’d like to complete the entire series of five classes in this specialization and earn the certificate.</del>\n",
        "- Brainstorm direction of KeyStone Artificial Intelligence Development (KeyAID) project: is this a periodic Professional Services engagement?  Bring a credit union in on pilot for free, and have them help us collect the data more easily by loading a backup from six months ago and another from a year ago, for example.  One possible deliverable is a combination of a SQL query of open loans, a prediction based on an already trained model, and a report of loans to review.  Naturally, these would be combined into a single batch program.  Also figure out how to market.\n",
        "- Once we have access to live data with some backups, simplify SQL queries used for learning, and do some additional work with feature engineering and looking at false negatives for “inspiration” on new features that can improve the model above the level reached during prototyping.  Note: new queries will not have as many charged off loans to use for training (only those that were charged off between backup and current database), but prediction might still be better, since not all “bad” loans will be exactly six months from being charged off.\n",
        "- Brainstorm additional use cases for AI/ML, such as fraud detection, watch list screening, marketing, etc.  Look for most promising next use case based on a combination of client need, availability of data, and whether or not the solution is already essentially a commodity from established players (e.g. credit scores from the bureaus).\n",
        "\n",
        "\n",
        "[Return to top of Notebook](#introduction)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jnv6ZnH-WOyW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}